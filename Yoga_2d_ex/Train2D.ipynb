{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNHJoybdq6t8JqLLBnQg4vD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Setup environment"],"metadata":{"id":"viuPwAT-IRL2"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"3wKYWEhCIFKl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696319566621,"user_tz":-420,"elapsed":3612,"user":{"displayName":"Nhật Trường Nguyễn","userId":"08022918682976675539"}},"outputId":"5f05fa35-68ee-4846-d930-98e0d6502531"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["#@title Connect drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["#@title Move path current\n","import os\n","path = '/content/drive/MyDrive/Colab Notebook/yoga2d'\n","\n","os.chdir(path)\n","os.getcwd()"],"metadata":{"id":"i3HEMf_SIUzJ","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1696319569836,"user_tz":-420,"elapsed":519,"user":{"displayName":"Nhật Trường Nguyễn","userId":"08022918682976675539"}},"outputId":"f2b37f98-4e1a-4a1a-b2fb-3e57d9183d90"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Colab Notebook/yoga2d'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["import os\n","import datetime\n","import psutil\n","import shutil\n","import itertools\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow import keras\n","from matplotlib import pyplot as plt\n","from contextlib import redirect_stdout\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn.metrics import (\n","    classification_report,\n","    confusion_matrix,\n","    f1_score,\n",")\n","from def_lib_2d import *\n","import truongmodel_2d"],"metadata":{"id":"BSxhRrE3IwyD","executionInfo":{"status":"ok","timestamp":1696319576456,"user_tz":-420,"elapsed":605,"user":{"displayName":"Nhật Trường Nguyễn","userId":"08022918682976675539"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["list_dir = [\n","    \"./log\",\n","    \"./model_training\",\n","    \"./figures\",\n","    \"./save_models\",\n","    \"./statistics\",\n","    \"./data_frame\",\n","    \"./autrain\",\n","    \"./auvalid\"\n","]\n","for d in list_dir:\n","    if not os.path.exists(d):\n","        os.makedirs(d)"],"metadata":{"id":"cK0IHTR5Ixnz","executionInfo":{"status":"ok","timestamp":1696319582030,"user_tz":-420,"elapsed":610,"user":{"displayName":"Nhật Trường Nguyễn","userId":"08022918682976675539"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["#@title Make dataframe\n","dataset_folder = 'yoga_pose'\n","folder_csv_path = \"data_frame\"\n","\n","for folder in os.listdir(dataset_folder):\n","    folder_path = os.path.join(dataset_folder, folder)\n","    make_df(folder_path, folder_csv_path)"],"metadata":{"id":"DM4mgkB2d3OC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696319592650,"user_tz":-420,"elapsed":8266,"user":{"displayName":"Nhật Trường Nguyễn","userId":"08022918682976675539"}},"outputId":"e07e5b13-c111-4e84-9226-b1ca872e3199"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Number sample of train: 3153\n","Number sample of test: 786\n"]}]},{"cell_type":"markdown","source":["# Train Conv2d"],"metadata":{"id":"7hcXi3pYI3rC"}},{"cell_type":"code","source":["model_name = 'conv2d'\n","img_size = 150\n","path_data = \"data_frame\"\n","test_size = 0.15\n","epochs = 80\n","batch_size = 32\n","e_patience = 10\n","\n","# Load data frame\n","(train_df, valid_df, test_df) = load_datafame(path_data, test_size)"],"metadata":{"id":"onlCXTv3I5J8","executionInfo":{"status":"ok","timestamp":1696319597518,"user_tz":-420,"elapsed":581,"user":{"displayName":"Nhật Trường Nguyễn","userId":"08022918682976675539"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Balane train and valid data\n","n = 400  # number of samples in each class\n","train_df, total_tr = balance(train_df, n, \"./autrain\", img_size)\n","# valid_df, total_v = balance(valid_df, n * 0.2, \"./auvalid\", img_size)\n","# au_tr_and_v = total_tr + total_v\n","au_tr_and_v = total_tr"],"metadata":{"id":"qXniqSteI8by","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696320390371,"user_tz":-420,"elapsed":788404,"user":{"displayName":"Nhật Trường Nguyễn","userId":"08022918682976675539"}},"outputId":"f0164776-2680-4aef-ae6c-3aa84f11bf0b"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Initial length of dataframe is  2680\n","Found 228 validated image filenames.\n","Found 154 validated image filenames.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Found 251 validated image filenames.\n","Found 340 validated image filenames.\n","Found 340 validated image filenames.\n","Found 340 validated image filenames.\n","Found 340 validated image filenames.\n","Found 265 validated image filenames.\n","Found 201 validated image filenames.\n","Found 221 validated image filenames.\n","Total Augmented images created=  1320\n","Length of augmented dataframe is now  4000\n"]}]},{"cell_type":"code","source":["#@title Conv2d 1 maxpoll [1:5]conv2d per maxpool\n","\n","num_maxpools = 1\n","max_num_conv_layers_per_maxpool = 5\n","# Iterate through the number of Conv1D layers per max-pooling layer\n","for num_conv_layers_per_maxpool in range(\n","    1, max_num_conv_layers_per_maxpool + 1\n","):\n","    print(\"=\" * 150)\n","    print(\n","        f\"{num_maxpools} maxpool, {num_conv_layers_per_maxpool} conv per maxpool\"\n","    )\n","    now = datetime.datetime.now()\n","    timestring = now.strftime(\n","        \"%Y-%m-%d_%H-%M-%S\"\n","    )  # https://strftime.org/\n","    name_saved = (\n","        str(model_name)\n","        + \"_tsize-\"\n","        + str(test_size)\n","        + \"_imgsize-\"\n","        + str(img_size)\n","        + \"_au-\"\n","        + str(au_tr_and_v)\n","        + \"_ep-\"\n","        + str(epochs)\n","        + \"_bs-\"\n","        + str(batch_size)\n","        + \"_epp-\"\n","        + str(e_patience)\n","        + \"_\"\n","        + str(timestring)\n","    )\n","    run_exp(\n","        name_saved=name_saved,\n","        model_name=model_name,\n","        train_df=train_df,\n","        valid_df=valid_df,\n","        test_df=test_df,\n","        img_size=img_size,\n","        epochs=epochs,\n","        batch_size=batch_size,\n","        e_patience=e_patience,\n","        num_conv_layers_per_maxpool=num_conv_layers_per_maxpool,\n","        num_maxpools=num_maxpools,\n","    )"],"metadata":{"cellView":"form","id":"JnSvWIn7JHpM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Conv2d 2 maxpoll [1:5]conv2d per maxpool\n","\n","num_maxpools = 2\n","max_num_conv_layers_per_maxpool = 5\n","# Iterate through the number of Conv1D layers per max-pooling layer\n","for num_conv_layers_per_maxpool in range(\n","    1, max_num_conv_layers_per_maxpool + 1\n","):\n","    print(\"=\" * 150)\n","    print(\n","        f\"{num_maxpools} maxpool, {num_conv_layers_per_maxpool} conv per maxpool\"\n","    )\n","    now = datetime.datetime.now()\n","    timestring = now.strftime(\n","        \"%Y-%m-%d_%H-%M-%S\"\n","    )  # https://strftime.org/\n","    name_saved = (\n","        str(model_name)\n","        + \"_tsize-\"\n","        + str(test_size)\n","        + \"_imgsize-\"\n","        + str(img_size)\n","        + \"_au-\"\n","        + str(au_tr_and_v)\n","        + \"_ep-\"\n","        + str(epochs)\n","        + \"_bs-\"\n","        + str(batch_size)\n","        + \"_epp-\"\n","        + str(e_patience)\n","        + \"_\"\n","        + str(timestring)\n","    )\n","    run_exp(\n","        name_saved=name_saved,\n","        model_name=model_name,\n","        train_df=train_df,\n","        valid_df=valid_df,\n","        test_df=test_df,\n","        img_size=img_size,\n","        epochs=epochs,\n","        batch_size=batch_size,\n","        e_patience=e_patience,\n","        num_conv_layers_per_maxpool=num_conv_layers_per_maxpool,\n","        num_maxpools=num_maxpools,\n","    )"],"metadata":{"id":"sXRGOWp1JJK3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696328008599,"user_tz":-420,"elapsed":7618233,"user":{"displayName":"Nhật Trường Nguyễn","userId":"08022918682976675539"}},"outputId":"10815bee-19b1-40d8-84fd-74f7feaf6ace"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================================================================================================\n","2 maxpool, 1 conv per maxpool\n","Found 4000 validated image filenames belonging to 10 classes.\n","Found 473 validated image filenames belonging to 10 classes.\n","Found 786 validated image filenames belonging to 10 classes.\n","test batch size:  6   test steps:  131  number of classes :  10\n","Epoch 1/80\n"," 13/125 [==>...........................] - ETA: 14:18 - loss: 2.4832 - accuracy: 0.0986"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["125/125 [==============================] - ETA: 0s - loss: 2.2256 - accuracy: 0.1895\n","Epoch 1: val_accuracy improved from -inf to 0.37421, saving model to save_models/weights.best_conv2d-2maxpool-1convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-06-30.hdf5\n","125/125 [==============================] - 1321s 11s/step - loss: 2.2256 - accuracy: 0.1895 - val_loss: 1.9633 - val_accuracy: 0.3742\n","Epoch 2/80\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["125/125 [==============================] - ETA: 0s - loss: 1.8599 - accuracy: 0.3615\n","Epoch 2: val_accuracy improved from 0.37421 to 0.47780, saving model to save_models/weights.best_conv2d-2maxpool-1convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-06-30.hdf5\n","125/125 [==============================] - 28s 224ms/step - loss: 1.8599 - accuracy: 0.3615 - val_loss: 1.7589 - val_accuracy: 0.4778\n","Epoch 3/80\n","125/125 [==============================] - ETA: 0s - loss: 1.5758 - accuracy: 0.4712\n","Epoch 3: val_accuracy improved from 0.47780 to 0.53911, saving model to save_models/weights.best_conv2d-2maxpool-1convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-06-30.hdf5\n","125/125 [==============================] - 27s 218ms/step - loss: 1.5758 - accuracy: 0.4712 - val_loss: 1.5140 - val_accuracy: 0.5391\n","Epoch 4/80\n","125/125 [==============================] - ETA: 0s - loss: 1.3281 - accuracy: 0.5543\n","Epoch 4: val_accuracy improved from 0.53911 to 0.56871, saving model to save_models/weights.best_conv2d-2maxpool-1convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-06-30.hdf5\n","125/125 [==============================] - 28s 226ms/step - loss: 1.3281 - accuracy: 0.5543 - val_loss: 1.3774 - val_accuracy: 0.5687\n","Epoch 5/80\n","125/125 [==============================] - ETA: 0s - loss: 1.0850 - accuracy: 0.6285\n","Epoch 5: val_accuracy improved from 0.56871 to 0.59619, saving model to save_models/weights.best_conv2d-2maxpool-1convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-06-30.hdf5\n","125/125 [==============================] - 26s 210ms/step - loss: 1.0850 - accuracy: 0.6285 - val_loss: 1.3209 - val_accuracy: 0.5962\n","Epoch 6/80\n","125/125 [==============================] - ETA: 0s - loss: 0.8715 - accuracy: 0.7072\n","Epoch 6: val_accuracy improved from 0.59619 to 0.61734, saving model to save_models/weights.best_conv2d-2maxpool-1convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-06-30.hdf5\n","125/125 [==============================] - 27s 214ms/step - loss: 0.8715 - accuracy: 0.7072 - val_loss: 1.2557 - val_accuracy: 0.6173\n","Epoch 7/80\n","125/125 [==============================] - ETA: 0s - loss: 0.6994 - accuracy: 0.7640\n","Epoch 7: val_accuracy improved from 0.61734 to 0.62791, saving model to save_models/weights.best_conv2d-2maxpool-1convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-06-30.hdf5\n","125/125 [==============================] - 29s 231ms/step - loss: 0.6994 - accuracy: 0.7640 - val_loss: 1.2313 - val_accuracy: 0.6279\n","Epoch 8/80\n","125/125 [==============================] - ETA: 0s - loss: 0.5620 - accuracy: 0.8140\n","Epoch 8: val_accuracy did not improve from 0.62791\n","125/125 [==============================] - 26s 206ms/step - loss: 0.5620 - accuracy: 0.8140 - val_loss: 1.2628 - val_accuracy: 0.6131\n","Epoch 9/80\n","125/125 [==============================] - ETA: 0s - loss: 0.4795 - accuracy: 0.8342\n","Epoch 9: val_accuracy improved from 0.62791 to 0.63425, saving model to save_models/weights.best_conv2d-2maxpool-1convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-06-30.hdf5\n","125/125 [==============================] - 27s 216ms/step - loss: 0.4795 - accuracy: 0.8342 - val_loss: 1.2409 - val_accuracy: 0.6342\n","Epoch 10/80\n","125/125 [==============================] - ETA: 0s - loss: 0.4108 - accuracy: 0.8608\n","Epoch 10: val_accuracy did not improve from 0.63425\n","125/125 [==============================] - 26s 206ms/step - loss: 0.4108 - accuracy: 0.8608 - val_loss: 1.3100 - val_accuracy: 0.6195\n","Epoch 11/80\n","125/125 [==============================] - ETA: 0s - loss: 0.3571 - accuracy: 0.8758\n","Epoch 11: val_accuracy did not improve from 0.63425\n","125/125 [==============================] - 27s 218ms/step - loss: 0.3571 - accuracy: 0.8758 - val_loss: 1.3473 - val_accuracy: 0.6258\n","Epoch 12/80\n","125/125 [==============================] - ETA: 0s - loss: 0.3216 - accuracy: 0.8945\n","Epoch 12: val_accuracy did not improve from 0.63425\n","125/125 [==============================] - 27s 216ms/step - loss: 0.3216 - accuracy: 0.8945 - val_loss: 1.3319 - val_accuracy: 0.6258\n","Epoch 13/80\n","125/125 [==============================] - ETA: 0s - loss: 0.2783 - accuracy: 0.9065\n","Epoch 13: val_accuracy did not improve from 0.63425\n","125/125 [==============================] - 27s 216ms/step - loss: 0.2783 - accuracy: 0.9065 - val_loss: 1.4108 - val_accuracy: 0.6258\n","Epoch 14/80\n","125/125 [==============================] - ETA: 0s - loss: 0.2394 - accuracy: 0.9185\n","Epoch 14: val_accuracy improved from 0.63425 to 0.63848, saving model to save_models/weights.best_conv2d-2maxpool-1convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-06-30.hdf5\n","125/125 [==============================] - 28s 225ms/step - loss: 0.2394 - accuracy: 0.9185 - val_loss: 1.4347 - val_accuracy: 0.6385\n","Epoch 15/80\n","125/125 [==============================] - ETA: 0s - loss: 0.2341 - accuracy: 0.9247\n","Epoch 15: val_accuracy did not improve from 0.63848\n","125/125 [==============================] - 26s 205ms/step - loss: 0.2341 - accuracy: 0.9247 - val_loss: 1.5069 - val_accuracy: 0.6110\n","Epoch 16/80\n","125/125 [==============================] - ETA: 0s - loss: 0.2140 - accuracy: 0.9310\n","Epoch 16: val_accuracy did not improve from 0.63848\n","125/125 [==============================] - 26s 206ms/step - loss: 0.2140 - accuracy: 0.9310 - val_loss: 1.4819 - val_accuracy: 0.6321\n","Epoch 17/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1943 - accuracy: 0.9365\n","Epoch 17: val_accuracy did not improve from 0.63848\n","125/125 [==============================] - 25s 202ms/step - loss: 0.1943 - accuracy: 0.9365 - val_loss: 1.5128 - val_accuracy: 0.6279\n","Epoch 18/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1823 - accuracy: 0.9405\n","Epoch 18: val_accuracy did not improve from 0.63848\n","125/125 [==============================] - 25s 203ms/step - loss: 0.1823 - accuracy: 0.9405 - val_loss: 1.4577 - val_accuracy: 0.6258\n","Epoch 19/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1759 - accuracy: 0.9433\n","Epoch 19: val_accuracy did not improve from 0.63848\n","125/125 [==============================] - 27s 215ms/step - loss: 0.1759 - accuracy: 0.9433 - val_loss: 1.5928 - val_accuracy: 0.6110\n","Epoch 20/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1567 - accuracy: 0.9485\n","Epoch 20: val_accuracy did not improve from 0.63848\n","125/125 [==============================] - 27s 214ms/step - loss: 0.1567 - accuracy: 0.9485 - val_loss: 1.5543 - val_accuracy: 0.6279\n","Epoch 21/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1687 - accuracy: 0.9445\n","Epoch 21: val_accuracy did not improve from 0.63848\n","125/125 [==============================] - 26s 212ms/step - loss: 0.1687 - accuracy: 0.9445 - val_loss: 1.5862 - val_accuracy: 0.6364\n","Epoch 22/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1412 - accuracy: 0.9520\n","Epoch 22: val_accuracy did not improve from 0.63848\n","125/125 [==============================] - 27s 217ms/step - loss: 0.1412 - accuracy: 0.9520 - val_loss: 1.5740 - val_accuracy: 0.6216\n","Epoch 23/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1321 - accuracy: 0.9565\n","Epoch 23: val_accuracy improved from 0.63848 to 0.64693, saving model to save_models/weights.best_conv2d-2maxpool-1convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-06-30.hdf5\n","125/125 [==============================] - 28s 221ms/step - loss: 0.1321 - accuracy: 0.9565 - val_loss: 1.5468 - val_accuracy: 0.6469\n","Epoch 24/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1433 - accuracy: 0.9505\n","Epoch 24: val_accuracy did not improve from 0.64693\n","125/125 [==============================] - 27s 215ms/step - loss: 0.1433 - accuracy: 0.9505 - val_loss: 1.7506 - val_accuracy: 0.6068\n","Epoch 25/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1375 - accuracy: 0.9520\n","Epoch 25: val_accuracy did not improve from 0.64693\n","125/125 [==============================] - 25s 202ms/step - loss: 0.1375 - accuracy: 0.9520 - val_loss: 1.5385 - val_accuracy: 0.6427\n","Epoch 26/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1234 - accuracy: 0.9595\n","Epoch 26: val_accuracy did not improve from 0.64693\n","125/125 [==============================] - 27s 218ms/step - loss: 0.1234 - accuracy: 0.9595 - val_loss: 1.6232 - val_accuracy: 0.6406\n","Epoch 27/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1279 - accuracy: 0.9570\n","Epoch 27: val_accuracy did not improve from 0.64693\n","125/125 [==============================] - 26s 204ms/step - loss: 0.1279 - accuracy: 0.9570 - val_loss: 1.6419 - val_accuracy: 0.6385\n","Epoch 28/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1260 - accuracy: 0.9590\n","Epoch 28: val_accuracy did not improve from 0.64693\n","125/125 [==============================] - 25s 200ms/step - loss: 0.1260 - accuracy: 0.9590 - val_loss: 1.6765 - val_accuracy: 0.6342\n","Epoch 29/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1235 - accuracy: 0.9613\n","Epoch 29: val_accuracy did not improve from 0.64693\n","125/125 [==============================] - 25s 202ms/step - loss: 0.1235 - accuracy: 0.9613 - val_loss: 1.6847 - val_accuracy: 0.6300\n","Epoch 30/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1116 - accuracy: 0.9638\n","Epoch 30: val_accuracy did not improve from 0.64693\n","125/125 [==============================] - 26s 204ms/step - loss: 0.1116 - accuracy: 0.9638 - val_loss: 1.7615 - val_accuracy: 0.6237\n","Epoch 31/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1016 - accuracy: 0.9638\n","Epoch 31: val_accuracy did not improve from 0.64693\n","125/125 [==============================] - 26s 205ms/step - loss: 0.1016 - accuracy: 0.9638 - val_loss: 1.7211 - val_accuracy: 0.6427\n","Epoch 32/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1005 - accuracy: 0.9655\n","Epoch 32: val_accuracy did not improve from 0.64693\n","125/125 [==============================] - 26s 204ms/step - loss: 0.1005 - accuracy: 0.9655 - val_loss: 1.7970 - val_accuracy: 0.6342\n","Epoch 33/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1114 - accuracy: 0.9607\n","Epoch 33: val_accuracy did not improve from 0.64693\n","125/125 [==============================] - 27s 215ms/step - loss: 0.1114 - accuracy: 0.9607 - val_loss: 1.6750 - val_accuracy: 0.6258\n","Memory used before training: 16.3%\n","Memory used after training: 34.7%\n","Memory used: 18.400000000000002 %\n","====================================================================================================\n","Samples training set: 4000\n","Samples validation set: 473\n","Samples test set: 786\n","Running time: 0:39:39.892191\n","Memory consumed during training (%): 18.400000000000002\n","Total epochs: 33\n","Best train accuracy: 0.965499997138977 / epoch: 32\n","Best val accuracy: 0.646934449672699 / epoch: 23\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":[" 21/125 [====>.........................] - ETA: 17s - loss: 0.0170 - accuracy: 0.9985"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["125/125 [==============================] - 21s 168ms/step - loss: 0.0190 - accuracy: 0.9995\n","131/131 [==============================] - 575s 4s/step - loss: 1.7012 - accuracy: 0.6209\n","LOSS TRAIN: 0.018950430676341057 / ACCURACY TRAIN: 0.9994999766349792\n","LOSS TEST: 1.7012273073196411 / ACCURACY TEST: 0.6208651661872864\n","131/131 [==============================] - 7s 54ms/step\n","Confusion matrix, without normalization\n","Normalized confusion matrix\n","\n","Classification Report:\n","                           precision    recall  f1-score   support\n","\n","              Chair_Pose       0.83      0.74      0.78        78\n","              Cobra_Pose       0.66      0.61      0.64       100\n","      Dolphin_Plank_Pose       0.31      0.40      0.35        45\n","Downward-Facing_Dog_Pose       0.78      0.68      0.73       100\n","              Plank_Pose       0.34      0.42      0.38        65\n","         Side_Plank_Pose       0.43      0.60      0.50        58\n","               Tree_Pose       0.76      0.70      0.73       100\n","        Warrior_III_Pose       0.66      0.73      0.69        66\n","         Warrior_II_Pose       0.73      0.67      0.70       100\n","          Warrior_I_Pose       0.58      0.49      0.53        74\n","\n","                accuracy                           0.62       786\n","               macro avg       0.61      0.60      0.60       786\n","            weighted avg       0.64      0.62      0.63       786\n","\n","======================================================================================================================================================\n","2 maxpool, 2 conv per maxpool\n","Found 4000 validated image filenames belonging to 10 classes.\n","Found 473 validated image filenames belonging to 10 classes.\n","Found 786 validated image filenames belonging to 10 classes.\n","test batch size:  6   test steps:  131  number of classes :  10\n","Epoch 1/80\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["125/125 [==============================] - ETA: 0s - loss: 2.3268 - accuracy: 0.1053\n","Epoch 1: val_accuracy improved from -inf to 0.11628, saving model to save_models/weights.best_conv2d-2maxpool-2convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-57-14.hdf5\n","125/125 [==============================] - 32s 225ms/step - loss: 2.3268 - accuracy: 0.1053 - val_loss: 2.3025 - val_accuracy: 0.1163\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/80\n","125/125 [==============================] - ETA: 0s - loss: 2.2854 - accuracy: 0.1318\n","Epoch 2: val_accuracy improved from 0.11628 to 0.31501, saving model to save_models/weights.best_conv2d-2maxpool-2convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-57-14.hdf5\n","125/125 [==============================] - 29s 232ms/step - loss: 2.2854 - accuracy: 0.1318 - val_loss: 2.1531 - val_accuracy: 0.3150\n","Epoch 3/80\n","125/125 [==============================] - ETA: 0s - loss: 2.1052 - accuracy: 0.2480\n","Epoch 3: val_accuracy improved from 0.31501 to 0.39958, saving model to save_models/weights.best_conv2d-2maxpool-2convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-57-14.hdf5\n","125/125 [==============================] - 30s 239ms/step - loss: 2.1052 - accuracy: 0.2480 - val_loss: 1.9635 - val_accuracy: 0.3996\n","Epoch 4/80\n","125/125 [==============================] - ETA: 0s - loss: 1.8656 - accuracy: 0.3428\n","Epoch 4: val_accuracy improved from 0.39958 to 0.45243, saving model to save_models/weights.best_conv2d-2maxpool-2convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-57-14.hdf5\n","125/125 [==============================] - 29s 230ms/step - loss: 1.8656 - accuracy: 0.3428 - val_loss: 1.7346 - val_accuracy: 0.4524\n","Epoch 5/80\n","125/125 [==============================] - ETA: 0s - loss: 1.6228 - accuracy: 0.4288\n","Epoch 5: val_accuracy improved from 0.45243 to 0.54334, saving model to save_models/weights.best_conv2d-2maxpool-2convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-57-14.hdf5\n","125/125 [==============================] - 29s 229ms/step - loss: 1.6228 - accuracy: 0.4288 - val_loss: 1.5159 - val_accuracy: 0.5433\n","Epoch 6/80\n","125/125 [==============================] - ETA: 0s - loss: 1.2978 - accuracy: 0.5312\n","Epoch 6: val_accuracy improved from 0.54334 to 0.56871, saving model to save_models/weights.best_conv2d-2maxpool-2convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-57-14.hdf5\n","125/125 [==============================] - 28s 225ms/step - loss: 1.2978 - accuracy: 0.5312 - val_loss: 1.4027 - val_accuracy: 0.5687\n","Epoch 7/80\n","125/125 [==============================] - ETA: 0s - loss: 1.0266 - accuracy: 0.6195\n","Epoch 7: val_accuracy improved from 0.56871 to 0.59408, saving model to save_models/weights.best_conv2d-2maxpool-2convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-57-14.hdf5\n","125/125 [==============================] - 29s 230ms/step - loss: 1.0266 - accuracy: 0.6195 - val_loss: 1.2661 - val_accuracy: 0.5941\n","Epoch 8/80\n","125/125 [==============================] - ETA: 0s - loss: 0.8040 - accuracy: 0.7113\n","Epoch 8: val_accuracy improved from 0.59408 to 0.61311, saving model to save_models/weights.best_conv2d-2maxpool-2convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-57-14.hdf5\n","125/125 [==============================] - 29s 227ms/step - loss: 0.8040 - accuracy: 0.7113 - val_loss: 1.1993 - val_accuracy: 0.6131\n","Epoch 9/80\n","125/125 [==============================] - ETA: 0s - loss: 0.6551 - accuracy: 0.7640\n","Epoch 9: val_accuracy improved from 0.61311 to 0.61945, saving model to save_models/weights.best_conv2d-2maxpool-2convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-57-14.hdf5\n","125/125 [==============================] - 29s 234ms/step - loss: 0.6551 - accuracy: 0.7640 - val_loss: 1.2197 - val_accuracy: 0.6195\n","Epoch 10/80\n","125/125 [==============================] - ETA: 0s - loss: 0.5733 - accuracy: 0.7878\n","Epoch 10: val_accuracy did not improve from 0.61945\n","125/125 [==============================] - 28s 220ms/step - loss: 0.5733 - accuracy: 0.7878 - val_loss: 1.2941 - val_accuracy: 0.6025\n","Epoch 11/80\n","125/125 [==============================] - ETA: 0s - loss: 0.5012 - accuracy: 0.8140\n","Epoch 11: val_accuracy did not improve from 0.61945\n","125/125 [==============================] - 27s 219ms/step - loss: 0.5012 - accuracy: 0.8140 - val_loss: 1.2894 - val_accuracy: 0.6047\n","Epoch 12/80\n","125/125 [==============================] - ETA: 0s - loss: 0.4145 - accuracy: 0.8525\n","Epoch 12: val_accuracy improved from 0.61945 to 0.63848, saving model to save_models/weights.best_conv2d-2maxpool-2convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-57-14.hdf5\n","125/125 [==============================] - 29s 235ms/step - loss: 0.4145 - accuracy: 0.8525 - val_loss: 1.3102 - val_accuracy: 0.6385\n","Epoch 13/80\n","125/125 [==============================] - ETA: 0s - loss: 0.3847 - accuracy: 0.8577\n","Epoch 13: val_accuracy did not improve from 0.63848\n","125/125 [==============================] - 27s 219ms/step - loss: 0.3847 - accuracy: 0.8577 - val_loss: 1.3750 - val_accuracy: 0.6110\n","Epoch 14/80\n","125/125 [==============================] - ETA: 0s - loss: 0.3269 - accuracy: 0.8767\n","Epoch 14: val_accuracy did not improve from 0.63848\n","125/125 [==============================] - 27s 217ms/step - loss: 0.3269 - accuracy: 0.8767 - val_loss: 1.4240 - val_accuracy: 0.6342\n","Epoch 15/80\n","125/125 [==============================] - ETA: 0s - loss: 0.3425 - accuracy: 0.8675\n","Epoch 15: val_accuracy improved from 0.63848 to 0.64693, saving model to save_models/weights.best_conv2d-2maxpool-2convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-57-14.hdf5\n","125/125 [==============================] - 30s 236ms/step - loss: 0.3425 - accuracy: 0.8675 - val_loss: 1.4210 - val_accuracy: 0.6469\n","Epoch 16/80\n","125/125 [==============================] - ETA: 0s - loss: 0.2952 - accuracy: 0.8888\n","Epoch 16: val_accuracy did not improve from 0.64693\n","125/125 [==============================] - 27s 218ms/step - loss: 0.2952 - accuracy: 0.8888 - val_loss: 1.4947 - val_accuracy: 0.6406\n","Epoch 17/80\n","125/125 [==============================] - ETA: 0s - loss: 0.2890 - accuracy: 0.8980\n","Epoch 17: val_accuracy improved from 0.64693 to 0.64905, saving model to save_models/weights.best_conv2d-2maxpool-2convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-57-14.hdf5\n","125/125 [==============================] - 28s 222ms/step - loss: 0.2890 - accuracy: 0.8980 - val_loss: 1.4224 - val_accuracy: 0.6490\n","Epoch 18/80\n","125/125 [==============================] - ETA: 0s - loss: 0.2499 - accuracy: 0.9078\n","Epoch 18: val_accuracy did not improve from 0.64905\n","125/125 [==============================] - 27s 217ms/step - loss: 0.2499 - accuracy: 0.9078 - val_loss: 1.5684 - val_accuracy: 0.6448\n","Epoch 19/80\n","125/125 [==============================] - ETA: 0s - loss: 0.2704 - accuracy: 0.8988\n","Epoch 19: val_accuracy improved from 0.64905 to 0.65328, saving model to save_models/weights.best_conv2d-2maxpool-2convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-57-14.hdf5\n","125/125 [==============================] - 28s 226ms/step - loss: 0.2704 - accuracy: 0.8988 - val_loss: 1.5239 - val_accuracy: 0.6533\n","Epoch 20/80\n","125/125 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.9100\n","Epoch 20: val_accuracy improved from 0.65328 to 0.66385, saving model to save_models/weights.best_conv2d-2maxpool-2convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-57-14.hdf5\n","125/125 [==============================] - 28s 225ms/step - loss: 0.2487 - accuracy: 0.9100 - val_loss: 1.5731 - val_accuracy: 0.6638\n","Epoch 21/80\n","125/125 [==============================] - ETA: 0s - loss: 0.2330 - accuracy: 0.9120\n","Epoch 21: val_accuracy did not improve from 0.66385\n","125/125 [==============================] - 28s 226ms/step - loss: 0.2330 - accuracy: 0.9120 - val_loss: 1.7224 - val_accuracy: 0.6321\n","Epoch 22/80\n","125/125 [==============================] - ETA: 0s - loss: 0.2208 - accuracy: 0.9118\n","Epoch 22: val_accuracy did not improve from 0.66385\n","125/125 [==============================] - 28s 224ms/step - loss: 0.2208 - accuracy: 0.9118 - val_loss: 1.5846 - val_accuracy: 0.6596\n","Epoch 23/80\n","125/125 [==============================] - ETA: 0s - loss: 0.2103 - accuracy: 0.9178\n","Epoch 23: val_accuracy did not improve from 0.66385\n","125/125 [==============================] - 28s 221ms/step - loss: 0.2103 - accuracy: 0.9178 - val_loss: 1.5463 - val_accuracy: 0.6554\n","Epoch 24/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1987 - accuracy: 0.9290\n","Epoch 24: val_accuracy did not improve from 0.66385\n","125/125 [==============================] - 28s 227ms/step - loss: 0.1987 - accuracy: 0.9290 - val_loss: 1.6974 - val_accuracy: 0.6427\n","Epoch 25/80\n","125/125 [==============================] - ETA: 0s - loss: 0.2137 - accuracy: 0.9190\n","Epoch 25: val_accuracy did not improve from 0.66385\n","125/125 [==============================] - 27s 216ms/step - loss: 0.2137 - accuracy: 0.9190 - val_loss: 1.5781 - val_accuracy: 0.6512\n","Epoch 26/80\n","125/125 [==============================] - ETA: 0s - loss: 0.2195 - accuracy: 0.9225\n","Epoch 26: val_accuracy improved from 0.66385 to 0.66596, saving model to save_models/weights.best_conv2d-2maxpool-2convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-57-14.hdf5\n","125/125 [==============================] - 29s 230ms/step - loss: 0.2195 - accuracy: 0.9225 - val_loss: 1.4787 - val_accuracy: 0.6660\n","Epoch 27/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1885 - accuracy: 0.9323\n","Epoch 27: val_accuracy improved from 0.66596 to 0.67442, saving model to save_models/weights.best_conv2d-2maxpool-2convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_08-57-14.hdf5\n","125/125 [==============================] - 29s 233ms/step - loss: 0.1885 - accuracy: 0.9323 - val_loss: 1.6345 - val_accuracy: 0.6744\n","Epoch 28/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1905 - accuracy: 0.9293\n","Epoch 28: val_accuracy did not improve from 0.67442\n","125/125 [==============================] - 27s 216ms/step - loss: 0.1905 - accuracy: 0.9293 - val_loss: 1.6458 - val_accuracy: 0.6617\n","Epoch 29/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1747 - accuracy: 0.9325\n","Epoch 29: val_accuracy did not improve from 0.67442\n","125/125 [==============================] - 28s 227ms/step - loss: 0.1747 - accuracy: 0.9325 - val_loss: 1.5157 - val_accuracy: 0.6216\n","Epoch 30/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1775 - accuracy: 0.9325\n","Epoch 30: val_accuracy did not improve from 0.67442\n","125/125 [==============================] - 28s 225ms/step - loss: 0.1775 - accuracy: 0.9325 - val_loss: 1.5936 - val_accuracy: 0.6575\n","Epoch 31/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1667 - accuracy: 0.9365\n","Epoch 31: val_accuracy did not improve from 0.67442\n","125/125 [==============================] - 28s 228ms/step - loss: 0.1667 - accuracy: 0.9365 - val_loss: 1.6665 - val_accuracy: 0.6427\n","Epoch 32/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1761 - accuracy: 0.9355\n","Epoch 32: val_accuracy did not improve from 0.67442\n","125/125 [==============================] - 28s 226ms/step - loss: 0.1761 - accuracy: 0.9355 - val_loss: 1.5649 - val_accuracy: 0.6681\n","Epoch 33/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1453 - accuracy: 0.9438\n","Epoch 33: val_accuracy did not improve from 0.67442\n","125/125 [==============================] - 28s 224ms/step - loss: 0.1453 - accuracy: 0.9438 - val_loss: 1.8856 - val_accuracy: 0.6660\n","Epoch 34/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1708 - accuracy: 0.9390\n","Epoch 34: val_accuracy did not improve from 0.67442\n","125/125 [==============================] - 27s 216ms/step - loss: 0.1708 - accuracy: 0.9390 - val_loss: 1.5605 - val_accuracy: 0.6533\n","Epoch 35/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1554 - accuracy: 0.9425\n","Epoch 35: val_accuracy did not improve from 0.67442\n","125/125 [==============================] - 29s 231ms/step - loss: 0.1554 - accuracy: 0.9425 - val_loss: 1.7059 - val_accuracy: 0.6596\n","Epoch 36/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1584 - accuracy: 0.9435\n","Epoch 36: val_accuracy did not improve from 0.67442\n","125/125 [==============================] - 27s 219ms/step - loss: 0.1584 - accuracy: 0.9435 - val_loss: 1.6586 - val_accuracy: 0.6660\n","Epoch 37/80\n","125/125 [==============================] - ETA: 0s - loss: 0.1542 - accuracy: 0.9450\n","Epoch 37: val_accuracy did not improve from 0.67442\n","125/125 [==============================] - 29s 231ms/step - loss: 0.1542 - accuracy: 0.9450 - val_loss: 1.5304 - val_accuracy: 0.6702\n","Memory used before training: 34.9%\n","Memory used after training: 37.4%\n","Memory used: 2.5 %\n","====================================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Samples training set: 4000\n","Samples validation set: 473\n","Samples test set: 786\n","Running time: 0:19:45.884992\n","Memory consumed during training (%): 2.5\n","Total epochs: 37\n","Best train accuracy: 0.9449999928474426 / epoch: 37\n","Best val accuracy: 0.6744186282157898 / epoch: 27\n","  2/125 [..............................] - ETA: 18s - loss: 0.0219 - accuracy: 1.0000"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["125/125 [==============================] - 21s 172ms/step - loss: 0.0207 - accuracy: 0.9977\n","131/131 [==============================] - 6s 46ms/step - loss: 1.7447 - accuracy: 0.6336\n","LOSS TRAIN: 0.020749669522047043 / ACCURACY TRAIN: 0.9977499842643738\n","LOSS TEST: 1.7446750402450562 / ACCURACY TEST: 0.6335877776145935\n","131/131 [==============================] - 6s 49ms/step\n","Confusion matrix, without normalization\n","Normalized confusion matrix\n","\n","Classification Report:\n","                           precision    recall  f1-score   support\n","\n","              Chair_Pose       0.75      0.74      0.75        78\n","              Cobra_Pose       0.62      0.48      0.54       100\n","      Dolphin_Plank_Pose       0.31      0.38      0.34        45\n","Downward-Facing_Dog_Pose       0.85      0.78      0.81       100\n","              Plank_Pose       0.43      0.49      0.46        65\n","         Side_Plank_Pose       0.52      0.67      0.59        58\n","               Tree_Pose       0.80      0.70      0.74       100\n","        Warrior_III_Pose       0.63      0.71      0.67        66\n","         Warrior_II_Pose       0.76      0.68      0.72       100\n","          Warrior_I_Pose       0.49      0.55      0.52        74\n","\n","                accuracy                           0.63       786\n","               macro avg       0.62      0.62      0.61       786\n","            weighted avg       0.65      0.63      0.64       786\n","\n","======================================================================================================================================================\n","2 maxpool, 3 conv per maxpool\n","Found 4000 validated image filenames belonging to 10 classes.\n","Found 473 validated image filenames belonging to 10 classes.\n","Found 786 validated image filenames belonging to 10 classes.\n","test batch size:  6   test steps:  131  number of classes :  10\n","Epoch 1/80\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["125/125 [==============================] - ETA: 0s - loss: 2.3134 - accuracy: 0.0940\n","Epoch 1: val_accuracy improved from -inf to 0.08245, saving model to save_models/weights.best_conv2d-2maxpool-3convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_09-18-03.hdf5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r125/125 [==============================] - 45s 309ms/step - loss: 2.3134 - accuracy: 0.0940 - val_loss: 2.3024 - val_accuracy: 0.0825\n","Epoch 2/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3028 - accuracy: 0.0895\n","Epoch 2: val_accuracy improved from 0.08245 to 0.08457, saving model to save_models/weights.best_conv2d-2maxpool-3convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_09-18-03.hdf5\n","125/125 [==============================] - 37s 293ms/step - loss: 2.3028 - accuracy: 0.0895 - val_loss: 2.3026 - val_accuracy: 0.0846\n","Epoch 3/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0897\n","Epoch 3: val_accuracy improved from 0.08457 to 0.12685, saving model to save_models/weights.best_conv2d-2maxpool-3convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_09-18-03.hdf5\n","125/125 [==============================] - 39s 312ms/step - loss: 2.3027 - accuracy: 0.0897 - val_loss: 2.3026 - val_accuracy: 0.1268\n","Epoch 4/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3028 - accuracy: 0.0918\n","Epoch 4: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 37s 295ms/step - loss: 2.3028 - accuracy: 0.0918 - val_loss: 2.3026 - val_accuracy: 0.1268\n","Epoch 5/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0972\n","Epoch 5: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 35s 281ms/step - loss: 2.3027 - accuracy: 0.0972 - val_loss: 2.3026 - val_accuracy: 0.0951\n","Epoch 6/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0895\n","Epoch 6: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 35s 277ms/step - loss: 2.3027 - accuracy: 0.0895 - val_loss: 2.3026 - val_accuracy: 0.0846\n","Epoch 7/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0895\n","Epoch 7: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 37s 299ms/step - loss: 2.3027 - accuracy: 0.0895 - val_loss: 2.3026 - val_accuracy: 0.1268\n","Epoch 8/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0935\n","Epoch 8: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 35s 280ms/step - loss: 2.3027 - accuracy: 0.0935 - val_loss: 2.3026 - val_accuracy: 0.0994\n","Epoch 9/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0880\n","Epoch 9: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 35s 282ms/step - loss: 2.3027 - accuracy: 0.0880 - val_loss: 2.3027 - val_accuracy: 0.1268\n","Epoch 10/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0940\n","Epoch 10: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 37s 293ms/step - loss: 2.3027 - accuracy: 0.0940 - val_loss: 2.3025 - val_accuracy: 0.1268\n","Epoch 11/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0833\n","Epoch 11: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 35s 280ms/step - loss: 2.3027 - accuracy: 0.0833 - val_loss: 2.3026 - val_accuracy: 0.0825\n","Epoch 12/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0955\n","Epoch 12: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 37s 290ms/step - loss: 2.3027 - accuracy: 0.0955 - val_loss: 2.3026 - val_accuracy: 0.0994\n","Epoch 13/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0913\n","Epoch 13: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 35s 278ms/step - loss: 2.3027 - accuracy: 0.0913 - val_loss: 2.3026 - val_accuracy: 0.1268\n","Memory used before training: 36.9%\n","Memory used after training: 38.0%\n","Memory used: 1.1000000000000014 %\n","====================================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Samples training set: 4000\n","Samples validation set: 473\n","Samples test set: 786\n","Running time: 0:08:23.707417\n","Memory consumed during training (%): 1.1000000000000014\n","Total epochs: 13\n","Best train accuracy: 0.09724999964237213 / epoch: 5\n","Best val accuracy: 0.12684988975524902 / epoch: 3\n"," 11/125 [=>............................] - ETA: 16s - loss: 2.3026 - accuracy: 0.1080"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["125/125 [==============================] - 24s 193ms/step - loss: 2.3026 - accuracy: 0.1000\n","131/131 [==============================] - 8s 55ms/step - loss: 2.3026 - accuracy: 0.1272\n","LOSS TRAIN: 2.3025851249694824 / ACCURACY TRAIN: 0.10000000149011612\n","LOSS TEST: 2.302624464035034 / ACCURACY TEST: 0.12722645699977875\n","131/131 [==============================] - 9s 64ms/step\n","Confusion matrix, without normalization\n","Normalized confusion matrix\n","\n","Classification Report:\n","                           precision    recall  f1-score   support\n","\n","              Chair_Pose       0.00      0.00      0.00        78\n","              Cobra_Pose       0.00      0.00      0.00       100\n","      Dolphin_Plank_Pose       0.00      0.00      0.00        45\n","Downward-Facing_Dog_Pose       0.13      1.00      0.23       100\n","              Plank_Pose       0.00      0.00      0.00        65\n","         Side_Plank_Pose       0.00      0.00      0.00        58\n","               Tree_Pose       0.00      0.00      0.00       100\n","        Warrior_III_Pose       0.00      0.00      0.00        66\n","         Warrior_II_Pose       0.00      0.00      0.00       100\n","          Warrior_I_Pose       0.00      0.00      0.00        74\n","\n","                accuracy                           0.13       786\n","               macro avg       0.01      0.10      0.02       786\n","            weighted avg       0.02      0.13      0.03       786\n","\n","======================================================================================================================================================\n","2 maxpool, 4 conv per maxpool\n","Found 4000 validated image filenames belonging to 10 classes.\n","Found 473 validated image filenames belonging to 10 classes.\n","Found 786 validated image filenames belonging to 10 classes.\n","test batch size:  6   test steps:  131  number of classes :  10\n","Epoch 1/80\n","  6/125 [>.............................] - ETA: 1:17 - loss: 2.6325 - accuracy: 0.1146"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["125/125 [==============================] - ETA: 0s - loss: 2.3186 - accuracy: 0.0885\n","Epoch 1: val_accuracy improved from -inf to 0.08457, saving model to save_models/weights.best_conv2d-2maxpool-4convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_09-27-34.hdf5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r125/125 [==============================] - 93s 649ms/step - loss: 2.3186 - accuracy: 0.0885 - val_loss: 2.3028 - val_accuracy: 0.0846\n","Epoch 2/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0903\n","Epoch 2: val_accuracy did not improve from 0.08457\n","125/125 [==============================] - 75s 602ms/step - loss: 2.3027 - accuracy: 0.0903 - val_loss: 2.3028 - val_accuracy: 0.0571\n","Epoch 3/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0915\n","Epoch 3: val_accuracy did not improve from 0.08457\n","125/125 [==============================] - 76s 602ms/step - loss: 2.3027 - accuracy: 0.0915 - val_loss: 2.3027 - val_accuracy: 0.0846\n","Epoch 4/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0948\n","Epoch 4: val_accuracy improved from 0.08457 to 0.09937, saving model to save_models/weights.best_conv2d-2maxpool-4convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_09-27-34.hdf5\n","125/125 [==============================] - 83s 659ms/step - loss: 2.3027 - accuracy: 0.0948 - val_loss: 2.3027 - val_accuracy: 0.0994\n","Epoch 5/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0943\n","Epoch 5: val_accuracy did not improve from 0.09937\n","125/125 [==============================] - 76s 609ms/step - loss: 2.3027 - accuracy: 0.0943 - val_loss: 2.3027 - val_accuracy: 0.0571\n","Epoch 6/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0905\n","Epoch 6: val_accuracy improved from 0.09937 to 0.12685, saving model to save_models/weights.best_conv2d-2maxpool-4convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_09-27-34.hdf5\n","125/125 [==============================] - 80s 638ms/step - loss: 2.3027 - accuracy: 0.0905 - val_loss: 2.3026 - val_accuracy: 0.1268\n","Epoch 7/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0955\n","Epoch 7: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 77s 615ms/step - loss: 2.3027 - accuracy: 0.0955 - val_loss: 2.3026 - val_accuracy: 0.0571\n","Epoch 8/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0927\n","Epoch 8: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 75s 600ms/step - loss: 2.3027 - accuracy: 0.0927 - val_loss: 2.3027 - val_accuracy: 0.0571\n","Epoch 9/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0908\n","Epoch 9: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 75s 601ms/step - loss: 2.3027 - accuracy: 0.0908 - val_loss: 2.3025 - val_accuracy: 0.1268\n","Epoch 10/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0895\n","Epoch 10: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 76s 605ms/step - loss: 2.3027 - accuracy: 0.0895 - val_loss: 2.3026 - val_accuracy: 0.0740\n","Epoch 11/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0878\n","Epoch 11: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 75s 601ms/step - loss: 2.3027 - accuracy: 0.0878 - val_loss: 2.3026 - val_accuracy: 0.1268\n","Epoch 12/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0935\n","Epoch 12: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 77s 614ms/step - loss: 2.3027 - accuracy: 0.0935 - val_loss: 2.3025 - val_accuracy: 0.1268\n","Epoch 13/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0918\n","Epoch 13: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 75s 601ms/step - loss: 2.3027 - accuracy: 0.0918 - val_loss: 2.3027 - val_accuracy: 0.0571\n","Epoch 14/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0948\n","Epoch 14: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 75s 600ms/step - loss: 2.3027 - accuracy: 0.0948 - val_loss: 2.3027 - val_accuracy: 0.0571\n","Epoch 15/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0983\n","Epoch 15: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 75s 601ms/step - loss: 2.3027 - accuracy: 0.0983 - val_loss: 2.3025 - val_accuracy: 0.1268\n","Epoch 16/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0913\n","Epoch 16: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 77s 614ms/step - loss: 2.3027 - accuracy: 0.0913 - val_loss: 2.3026 - val_accuracy: 0.0994\n","Memory used before training: 39.2%\n","Memory used after training: 38.4%\n","Memory used: -0.8000000000000043 %\n","====================================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Samples training set: 4000\n","Samples validation set: 473\n","Samples test set: 786\n","Running time: 0:22:56.600102\n","Memory consumed during training (%): -0.8000000000000043\n","Total epochs: 16\n","Best train accuracy: 0.09825000166893005 / epoch: 15\n","Best val accuracy: 0.12684988975524902 / epoch: 6\n","  3/125 [..............................] - ETA: 19s - loss: 2.3026 - accuracy: 0.0729"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["125/125 [==============================] - 25s 200ms/step - loss: 2.3026 - accuracy: 0.1000\n","131/131 [==============================] - 8s 52ms/step - loss: 2.3026 - accuracy: 0.0992\n","LOSS TRAIN: 2.3025856018066406 / ACCURACY TRAIN: 0.10000000149011612\n","LOSS TEST: 2.302584648132324 / ACCURACY TEST: 0.09923664480447769\n","131/131 [==============================] - 8s 56ms/step\n","Confusion matrix, without normalization\n","Normalized confusion matrix\n","\n","Classification Report:\n","                           precision    recall  f1-score   support\n","\n","              Chair_Pose       0.10      1.00      0.18        78\n","              Cobra_Pose       0.00      0.00      0.00       100\n","      Dolphin_Plank_Pose       0.00      0.00      0.00        45\n","Downward-Facing_Dog_Pose       0.00      0.00      0.00       100\n","              Plank_Pose       0.00      0.00      0.00        65\n","         Side_Plank_Pose       0.00      0.00      0.00        58\n","               Tree_Pose       0.00      0.00      0.00       100\n","        Warrior_III_Pose       0.00      0.00      0.00        66\n","         Warrior_II_Pose       0.00      0.00      0.00       100\n","          Warrior_I_Pose       0.00      0.00      0.00        74\n","\n","                accuracy                           0.10       786\n","               macro avg       0.01      0.10      0.02       786\n","            weighted avg       0.01      0.10      0.02       786\n","\n","======================================================================================================================================================\n","2 maxpool, 5 conv per maxpool\n","Found 4000 validated image filenames belonging to 10 classes.\n","Found 473 validated image filenames belonging to 10 classes.\n","Found 786 validated image filenames belonging to 10 classes.\n","test batch size:  6   test steps:  131  number of classes :  10\n","Epoch 1/80\n"," 15/125 [==>...........................] - ETA: 1:31 - loss: 2.6014 - accuracy: 0.1083"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["125/125 [==============================] - ETA: 0s - loss: 2.3386 - accuracy: 0.0940\n","Epoch 1: val_accuracy improved from -inf to 0.12685, saving model to save_models/weights.best_conv2d-2maxpool-5convpermaxpool_tsize-0.15_imgsize-150_au-1320_ep-80_bs-32_epp-10_2023-10-03_09-51-20.hdf5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r125/125 [==============================] - 114s 826ms/step - loss: 2.3386 - accuracy: 0.0940 - val_loss: 2.3026 - val_accuracy: 0.1268\n","Epoch 2/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3028 - accuracy: 0.0855\n","Epoch 2: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 100s 797ms/step - loss: 2.3028 - accuracy: 0.0855 - val_loss: 2.3027 - val_accuracy: 0.0994\n","Epoch 3/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.1000\n","Epoch 3: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 99s 792ms/step - loss: 2.3027 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.0994\n","Epoch 4/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.1007\n","Epoch 4: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 100s 798ms/step - loss: 2.3027 - accuracy: 0.1007 - val_loss: 2.3027 - val_accuracy: 0.0994\n","Epoch 5/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0980\n","Epoch 5: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 99s 791ms/step - loss: 2.3027 - accuracy: 0.0980 - val_loss: 2.3028 - val_accuracy: 0.0994\n","Epoch 6/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0913\n","Epoch 6: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 98s 786ms/step - loss: 2.3027 - accuracy: 0.0913 - val_loss: 2.3027 - val_accuracy: 0.0740\n","Epoch 7/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0903\n","Epoch 7: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 98s 787ms/step - loss: 2.3027 - accuracy: 0.0903 - val_loss: 2.3027 - val_accuracy: 0.0740\n","Epoch 8/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0895\n","Epoch 8: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 98s 785ms/step - loss: 2.3027 - accuracy: 0.0895 - val_loss: 2.3028 - val_accuracy: 0.0740\n","Epoch 9/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3026 - accuracy: 0.0972\n","Epoch 9: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 98s 785ms/step - loss: 2.3026 - accuracy: 0.0972 - val_loss: 2.3027 - val_accuracy: 0.0740\n","Epoch 10/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0880\n","Epoch 10: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 99s 789ms/step - loss: 2.3027 - accuracy: 0.0880 - val_loss: 2.3028 - val_accuracy: 0.0740\n","Epoch 11/80\n","125/125 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0955\n","Epoch 11: val_accuracy did not improve from 0.12685\n","125/125 [==============================] - 99s 789ms/step - loss: 2.3027 - accuracy: 0.0955 - val_loss: 2.3028 - val_accuracy: 0.0740\n","Memory used before training: 38.6%\n","Memory used after training: 40.0%\n","Memory used: 1.3999999999999986 %\n","====================================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Samples training set: 4000\n","Samples validation set: 473\n","Samples test set: 786\n","Running time: 0:20:18.519202\n","Memory consumed during training (%): 1.3999999999999986\n","Total epochs: 11\n","Best train accuracy: 0.10074999928474426 / epoch: 4\n","Best val accuracy: 0.12684988975524902 / epoch: 1\n","  8/125 [>.............................] - ETA: 25s - loss: 2.3025 - accuracy: 0.1055"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["125/125 [==============================] - 25s 201ms/step - loss: 2.3026 - accuracy: 0.1000\n","131/131 [==============================] - 9s 65ms/step - loss: 2.3028 - accuracy: 0.0738\n","LOSS TRAIN: 2.302586078643799 / ACCURACY TRAIN: 0.10000000149011612\n","LOSS TEST: 2.3027591705322266 / ACCURACY TEST: 0.07379134744405746\n","131/131 [==============================] - 9s 68ms/step\n","Confusion matrix, without normalization\n","Normalized confusion matrix\n","\n","Classification Report:\n","                           precision    recall  f1-score   support\n","\n","              Chair_Pose       0.00      0.00      0.00        78\n","              Cobra_Pose       0.00      0.00      0.00       100\n","      Dolphin_Plank_Pose       0.00      0.00      0.00        45\n","Downward-Facing_Dog_Pose       0.00      0.00      0.00       100\n","              Plank_Pose       0.00      0.00      0.00        65\n","         Side_Plank_Pose       0.07      1.00      0.14        58\n","               Tree_Pose       0.00      0.00      0.00       100\n","        Warrior_III_Pose       0.00      0.00      0.00        66\n","         Warrior_II_Pose       0.00      0.00      0.00       100\n","          Warrior_I_Pose       0.00      0.00      0.00        74\n","\n","                accuracy                           0.07       786\n","               macro avg       0.01      0.10      0.01       786\n","            weighted avg       0.01      0.07      0.01       786\n","\n"]}]},{"cell_type":"code","source":["#@title Conv2d 3 maxpoll [1:5]conv2d per maxpool\n","\n","num_maxpools = 3\n","max_num_conv_layers_per_maxpool = 5\n","# Iterate through the number of Conv1D layers per max-pooling layer\n","for num_conv_layers_per_maxpool in range(\n","    1, max_num_conv_layers_per_maxpool + 1\n","):\n","    print(\"=\" * 150)\n","    print(\n","        f\"{num_maxpools} maxpool, {num_conv_layers_per_maxpool} conv per maxpool\"\n","    )\n","    now = datetime.datetime.now()\n","    timestring = now.strftime(\n","        \"%Y-%m-%d_%H-%M-%S\"\n","    )  # https://strftime.org/\n","    name_saved = (\n","        str(model_name)\n","        + \"_tsize-\"\n","        + str(test_size)\n","        + \"_imgsize-\"\n","        + str(img_size)\n","        + \"_au-\"\n","        + str(au_tr_and_v)\n","        + \"_ep-\"\n","        + str(epochs)\n","        + \"_bs-\"\n","        + str(batch_size)\n","        + \"_epp-\"\n","        + str(e_patience)\n","        + \"_\"\n","        + str(timestring)\n","    )\n","    run_exp(\n","        name_saved=name_saved,\n","        model_name=model_name,\n","        train_df=train_df,\n","        valid_df=valid_df,\n","        test_df=test_df,\n","        img_size=img_size,\n","        epochs=epochs,\n","        batch_size=batch_size,\n","        e_patience=e_patience,\n","        num_conv_layers_per_maxpool=num_conv_layers_per_maxpool,\n","        num_maxpools=num_maxpools,\n","    )"],"metadata":{"cellView":"form","id":"bF2U3jK8JJTi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Conv2d 4 maxpoll [1:5]conv2d per maxpool\n","\n","num_maxpools = 4\n","max_num_conv_layers_per_maxpool = 5\n","# Iterate through the number of Conv1D layers per max-pooling layer\n","for num_conv_layers_per_maxpool in range(\n","    1, max_num_conv_layers_per_maxpool + 1\n","):\n","    print(\"=\" * 150)\n","    print(\n","        f\"{num_maxpools} maxpool, {num_conv_layers_per_maxpool} conv per maxpool\"\n","    )\n","    now = datetime.datetime.now()\n","    timestring = now.strftime(\n","        \"%Y-%m-%d_%H-%M-%S\"\n","    )  # https://strftime.org/\n","    name_saved = (\n","        str(model_name)\n","        + \"_tsize-\"\n","        + str(test_size)\n","        + \"_imgsize-\"\n","        + str(img_size)\n","        + \"_au-\"\n","        + str(au_tr_and_v)\n","        + \"_ep-\"\n","        + str(epochs)\n","        + \"_bs-\"\n","        + str(batch_size)\n","        + \"_epp-\"\n","        + str(e_patience)\n","        + \"_\"\n","        + str(timestring)\n","    )\n","    run_exp(\n","        name_saved=name_saved,\n","        model_name=model_name,\n","        train_df=train_df,\n","        valid_df=valid_df,\n","        test_df=test_df,\n","        img_size=img_size,\n","        epochs=epochs,\n","        batch_size=batch_size,\n","        e_patience=e_patience,\n","        num_conv_layers_per_maxpool=num_conv_layers_per_maxpool,\n","        num_maxpools=num_maxpools,\n","    )"],"metadata":{"cellView":"form","id":"CWTgm3IpJJb2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Conv2d 5 maxpoll [1:5]conv2d per maxpool\n","\n","num_maxpools = 5\n","max_num_conv_layers_per_maxpool = 5\n","# Iterate through the number of Conv1D layers per max-pooling layer\n","for num_conv_layers_per_maxpool in range(\n","    1, max_num_conv_layers_per_maxpool + 1\n","):\n","    print(\"=\" * 150)\n","    print(\n","        f\"{num_maxpools} maxpool, {num_conv_layers_per_maxpool} conv per maxpool\"\n","    )\n","    now = datetime.datetime.now()\n","    timestring = now.strftime(\n","        \"%Y-%m-%d_%H-%M-%S\"\n","    )  # https://strftime.org/\n","    name_saved = (\n","        str(model_name)\n","        + \"_tsize-\"\n","        + str(test_size)\n","        + \"_imgsize-\"\n","        + str(img_size)\n","        + \"_au-\"\n","        + str(au_tr_and_v)\n","        + \"_ep-\"\n","        + str(epochs)\n","        + \"_bs-\"\n","        + str(batch_size)\n","        + \"_epp-\"\n","        + str(e_patience)\n","        + \"_\"\n","        + str(timestring)\n","    )\n","    run_exp(\n","        name_saved=name_saved,\n","        model_name=model_name,\n","        train_df=train_df,\n","        valid_df=valid_df,\n","        test_df=test_df,\n","        img_size=img_size,\n","        epochs=epochs,\n","        batch_size=batch_size,\n","        e_patience=e_patience,\n","        num_conv_layers_per_maxpool=num_conv_layers_per_maxpool,\n","        num_maxpools=num_maxpools,\n","    )"],"metadata":{"cellView":"form","id":"hJYZG1iZJJlT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train Fcnn2d 10dense"],"metadata":{"id":"SwiCr57s3Jw_"}},{"cell_type":"code","source":["model_name = \"fcnn2d\"\n","\n","img_size = 150\n","path_data = \"data_frame\"\n","test_size = 0.15\n","epochs = 80\n","batch_size = 32\n","e_patience = 10\n","\n","max_dense_layers = 10\n","for num_dense in range(1, max_dense_layers + 1):\n","    print(\"=\" * 150)\n","    print(f\"{num_dense} dense layer\")\n","    now = datetime.datetime.now()\n","    timestring = now.strftime(\"%Y-%m-%d_%H-%M-%S\")  # https://strftime.org/\n","    name_saved = (\n","        str(model_name)\n","        + \"_tsize-\"\n","        + str(test_size)\n","        + \"_imgsize-\"\n","        + str(img_size)\n","        + \"_au-\"\n","        + str(au_tr_and_v)\n","        + \"_ep-\"\n","        + str(epochs)\n","        + \"_bs-\"\n","        + str(batch_size)\n","        + \"_epp-\"\n","        + str(e_patience)\n","        + \"_\"\n","        + str(timestring)\n","    )\n","\n","    run_exp(\n","        name_saved=name_saved,\n","        model_name=model_name,\n","        train_df=train_df,\n","        valid_df=valid_df,\n","        test_df=test_df,\n","        img_size = img_size,\n","        epochs=epochs,\n","        batch_size=batch_size,\n","        e_patience=e_patience,\n","        num_dense_layers=num_dense,\n","    )"],"metadata":{"id":"-F0ffEgE3M9W"},"execution_count":null,"outputs":[]}]}